if (j <= nrow(M) && keep[j] && max(abs(M[i, 1:5] - M[j, 1:5])) < 1e-4) keep[j] = FALSE
}
}
M[keep, , drop = FALSE]
}
if (length(solns) == 0) stop("No feasible solutions found. Try expanding the start grid.")
# Robust verification in one chunk (uses "=" throughout)
# If needed: install.packages("nleqslv")
suppressPackageStartupMessages(library(nleqslv))
# ---------------------------
# Given empirical raw moments
# ---------------------------
M1 = 17
M2 = 305
M3 = 5832
M4 = 116061
M5 = 2385610
emp = c(M1, M2, M3, M4, M5)
# ---------------------------
# Normal raw moments m_k(mu, s2)
# ---------------------------
m1 = function(mu, s2) mu
m2 = function(mu, s2) mu^2 + s2
m3 = function(mu, s2) mu^3 + 3*mu*s2
m4 = function(mu, s2) mu^4 + 6*mu^2*s2 + 3*s2^2
m5 = function(mu, s2) mu^5 + 10*mu^3*s2 + 15*mu*s2^2
mix_mk = function(lambda, mu1, mu2, s1, s2, k) {
f = switch(as.character(k), "1" = m1, "2" = m2, "3" = m3, "4" = m4, "5" = m5)
lambda*f(mu1, s1^2) + (1 - lambda)*f(mu2, s2^2)
}
# -------------------------------------------------------
# Stable parameterization with bounds (unconstrained -> θ)
#   l = logistic(t) keeps 0<l<1
#   s1 = exp(a), s2 = exp(b) keep sigmas positive
# -------------------------------------------------------
theta_from_u = function(u) {
t = u[1]; mu1 = u[2]; mu2 = u[3]; a = u[4]; b = u[5]
lambda = 1/(1 + exp(-t))
s1 = exp(a); s2 = exp(b)
c(lambda = lambda, mu1 = mu1, mu2 = mu2, s1 = s1, s2 = s2)
}
# -------------------------------------------------------
# Scaled equations: (predicted M_k - empirical M_k)/M_k = 0
# Scaling avoids nasty conditioning from huge higher moments
# -------------------------------------------------------
F_scaled = function(u) {
th = theta_from_u(u)
pred = sapply(1:5, function(k) mix_mk(th["lambda"], th["mu1"], th["mu2"], th["s1"], th["s2"], k))
(pred - emp)/emp
}
# -------------------------------------------------------
# A strong, feasible starting point (tiny sigma on one comp)
#   These come from a coarse search; they make the solver
#   converge quickly even if your previous grid did not.
# -------------------------------------------------------
lambda0 = 0.8656
mu10 = 16.0007
mu20 = 23.3719
s10 = 3.7522
s20 = 0.1039
u0 = c(qlogis(lambda0), mu10, mu20, log(s10), log(s20))
# Solve
fit = nleqslv(u0, F_scaled, method = "Broyden",
control = list(maxit = 500, ftol = 1e-14, xtol = 1e-12))
stopifnot(fit$termcd == 1)  # error if not converged
# Robust search + polish (one chunk, uses "="). Finds a feasible solution even if previous runs failed.
suppressPackageStartupMessages(library(nleqslv))
# ---------------------------
# Given empirical raw moments
# ---------------------------
M1 = 17
M2 = 305
M3 = 5832
M4 = 116061
M5 = 2385610
emp = c(M1, M2, M3, M4, M5)
# ---------------------------
# Normal raw moments m_k(mu, s2)
# ---------------------------
m1 = function(mu, s2) mu
m2 = function(mu, s2) mu^2 + s2
m3 = function(mu, s2) mu^3 + 3*mu*s2
m4 = function(mu, s2) mu^4 + 6*mu^2*s2 + 3*s2^2
m5 = function(mu, s2) mu^5 + 10*mu^3*s2 + 15*mu*s2^2
mix_mk = function(lambda, mu1, mu2, s1, s2, k) {
f = switch(as.character(k), "1" = m1, "2" = m2, "3" = m3, "4" = m4, "5" = m5)
lambda*f(mu1, s1^2) + (1 - lambda)*f(mu2, s2^2)
}
# ---------------------------
# Stable parameterization/bounds via unconstrained u
# ---------------------------
theta_from_u = function(u) {
t = u[1]; mu1 = u[2]; mu2 = u[3]; a = u[4]; b = u[5]
lambda = 1/(1 + exp(-t))          # in (0,1)
s1 = exp(a); s2 = exp(b)          # > 0
c(lambda = lambda, mu1 = mu1, mu2 = mu2, s1 = s1, s2 = s2)
}
# Scaled equations to improve conditioning
F_scaled = function(u) {
th = theta_from_u(u)
pred = sapply(1:5, function(k) mix_mk(th["lambda"], th["mu1"], th["mu2"], th["s1"], th["s2"], k))
(pred - emp)/emp
}
loss = function(u) sum(F_scaled(u)^2)
# ---------------------------
# Heuristic initializers
#   Use total mean/var to seed near-feasible zones, plus random restarts
# ---------------------------
set.seed(42)
mu_total = M1
var_total = M2 - M1^2
sd_total = sqrt(max(1e-8, var_total))  # ~4
# A small library of purposeful seeds
seed_params = list(
c( qlogis(0.7), mu_total - 1, mu_total + 7, log(sd_total), log(sd_total/5) ),
c( qlogis(0.3), mu_total - 6, mu_total + 2, log(sd_total/5), log(sd_total) ),
c( qlogis(0.5), mu_total - 1, mu_total + 6, log(sd_total*0.9), log(sd_total*0.1) ),
c( qlogis(0.85), mu_total - 1, mu_total + 6, log(sd_total), log(sd_total*0.05) ),
c( qlogis(0.15), mu_total - 6, mu_total + 1, log(sd_total*0.05), log(sd_total) )
)
# Add random restarts
n_random = 200
rand_starts = replicate(n_random, {
l = rnorm(1, 0, 1)                 # logistic will map this to (0,1)
mu1 = rnorm(1, mu_total - 2, 6)
mu2 = rnorm(1, mu_total + 6, 6)
a = rnorm(1, log(sd_total*runif(1, 0.05, 2.0)), 0.8)
b = rnorm(1, log(sd_total*runif(1, 0.05, 2.0)), 0.8)
c(l, mu1, mu2, a, b)
}, simplify = FALSE)
starts = c(seed_params, rand_starts)
# ---------------------------
# Global search with optim, then polish with nleqslv
# ---------------------------
best_u = NULL
best_loss = Inf
for (u0 in starts) {
opt = try(optim(u0, loss, method = "BFGS", control = list(reltol = 1e-12, maxit = 2000)), silent = TRUE)
if (inherits(opt, "try-error")) next
if (opt$value < best_loss && is.finite(opt$value)) {
best_loss = opt$value
best_u = opt$par
}
}
stopifnot(!is.null(best_u))
# Try to polish with a root-finder; if it fails, keep the optimizer result
polish = try(nleqslv(best_u, F_scaled, method = "Broyden",
control = list(maxit = 800, ftol = 1e-14, xtol = 1e-12)), silent = TRUE)
if (!inherits(polish, "try-error") && polish$termcd == 1 && sum(F_scaled(polish$x)^2) < best_loss) {
u_hat = polish$x
note = "Converged to a root (nleqslv)."
} else {
u_hat = best_u
note = "Used best least-squares solution (optim) because exact root was not found."
}
# Back-transform; impose an ordering mu1 <= mu2 for display (label-switching)
th = theta_from_u(u_hat)
lambda = th["lambda"]; mu1 = th["mu1"]; mu2 = th["mu2"]; s1 = th["s1"]; s2 = th["s2"]
if (mu1 > mu2) {
tmp_mu = mu1; mu1 = mu2; mu2 = tmp_mu
tmp_s = s1; s1 = s2; s2 = tmp_s
lambda = 1 - lambda
}
pred = sapply(1:5, function(k) mix_mk(lambda, mu1, mu2, s1, s2, k))
res = pred - emp
rel = res/emp
rss = sum((pred - emp)^2/emp^2)
cat(note, "\n")
cat(sprintf("lambda = %.6f\nmu1 = %.6f\nmu2 = %.6f\nsigma1 = %.6f\nsigma2 = %.6f\nRSS (relative) = %.3e\n\n",
lambda, mu1, mu2, s1, s2, rss))
cat("Raw-moment residuals (pred - emp):\n")
print(setNames(round(res, 6), paste0("M", 1:5)))
cat("\nRelative errors:\n")
print(setNames(signif(rel, 6), paste0("M", 1:5)))
# ---------------------------
# Quick comparison plot
# ---------------------------
op = par(no.readonly = TRUE)
on.exit(par(op), add = TRUE)
barplot(rbind(emp, pred),
beside = TRUE, names.arg = paste0("M", 1:5),
main = "Empirical vs Predicted Raw Moments (Orders 1–5)",
legend.text = c("Empirical", "Predicted"),
args.legend = list(x = "topleft", bty = "n"),
ylab = "value")
# Compact summary
out = data.frame(
moment = paste0("M", 1:5),
empirical = emp,
predicted = round(pred, 6),
abs_error = round(abs(pred - emp), 6),
rel_error = signif(rel, 6)
)
print(out, row.names = FALSE)
# Constrained verification (one chunk; uses "=")
# If needed: install.packages("stats")  # 'optim' is in stats
set.seed(1)
# Empirical raw moments
M1 = 17
M2 = 305
M3 = 5832
M4 = 116061
M5 = 2385610
emp = c(M1, M2, M3, M4, M5)
# Raw moments of N(mu, sigma^2)
m1 = function(mu, s2) mu
m2 = function(mu, s2) mu^2 + s2
m3 = function(mu, s2) mu^3 + 3*mu*s2
m4 = function(mu, s2) mu^4 + 6*mu^2*s2 + 3*s2^2
m5 = function(mu, s2) mu^5 + 10*mu^3*s2 + 15*mu*s2^2
mix_mk = function(lambda, mu1, mu2, s1, s2, k) {
f = switch(as.character(k), "1" = m1, "2" = m2, "3" = m3, "4" = m4, "5" = m5)
lambda*f(mu1, s1^2) + (1 - lambda)*f(mu2, s2^2)
}
# Objective: squared relative error of moments (orders 1–5) + small penalty discouraging near-bound sigmas
obj = function(theta) {
lambda = theta[1]; mu1 = theta[2]; mu2 = theta[3]; s1 = theta[4]; s2 = theta[5]
pred = sapply(1:5, function(k) mix_mk(lambda, mu1, mu2, s1, s2, k))
rel = (pred - emp)/emp
# gentle penalty to avoid sigma hugging the lower bound
pen = 1e-3 * (1/(s1^2) + 1/(s2^2))
sum(rel^2) + pen
}
# Bounds (tune if needed; sigmas bounded away from 0)
lower = c(  1e-3,   0,   0,  0.25,  0.25)
upper = c(1-1e-3,  40,  40,  40.00, 40.00)
# Heuristic seeds around overall mean/variance
mu_tot = M1
sd_tot = sqrt(max(1e-8, M2 - M1^2))
seed_bank = rbind(
c(0.7, mu_tot - 2, mu_tot + 6, sd_tot, sd_tot/3),
c(0.3, mu_tot - 6, mu_tot + 2, sd_tot/3, sd_tot),
c(0.5, mu_tot - 1, mu_tot + 5, sd_tot, sd_tot/5),
c(0.85, mu_tot - 1, mu_tot + 6, sd_tot, sd_tot/6),
c(0.15, mu_tot - 6, mu_tot + 1, sd_tot/6, sd_tot)
)
# Add random restarts within bounds
n_random = 200
rand = cbind(
runif(n_random, lower[1], upper[1]),
runif(n_random, lower[2], upper[2]),
runif(n_random, lower[3], upper[3]),
runif(n_random, lower[4], upper[4]),
runif(n_random, lower[5], upper[5])
)
starts = rbind(seed_bank, rand)
# Optimize from each start with bounds
best_val = Inf
best_par = NULL
for (i in seq_len(nrow(starts))) {
start = pmin(pmax(starts[i, ], lower), upper)
fit = try(optim(start, obj, method = "L-BFGS-B", lower = lower, upper = upper,
control = list(maxit = 5000, factr = 1e7)), silent = TRUE)
if (inherits(fit, "try-error")) next
if (is.finite(fit$value) && fit$value < best_val) {
best_val = fit$value
best_par = fit$par
}
}
stopifnot(!is.null(best_par))
# Enforce a canonical labeling (mu1 <= mu2)
lambda = best_par[1]; mu1 = best_par[2]; mu2 = best_par[3]; s1 = best_par[4]; s2 = best_par[5]
if (mu1 > mu2) { tmp = mu1; mu1 = mu2; mu2 = tmp; tmp = s1; s1 = s2; s2 = tmp; lambda = 1 - lambda }
# Quality report
pred = sapply(1:5, function(k) mix_mk(lambda, mu1, mu2, s1, s2, k))
rel = (pred - emp)/emp
rss_rel = sum(rel^2)
cat("Constrained least-squares fit (moments 1–5)\n")
cat(sprintf("lambda = %.6f\nmu1 = %.6f\nmu2 = %.6f\nsigma1 = %.6f\nsigma2 = %.6f\nRSS (relative) = %.3e\n\n",
lambda, mu1, mu2, s1, s2, rss_rel))
cat("Relative errors by moment:\n")
print(setNames(signif(rel, 6), paste0("M", 1:5)))
# Flag if solution is hugging bounds (esp. sigmas)
at_bound = (abs(s1 - lower[4]) < 1e-6) | (abs(s2 - lower[5]) < 1e-6) |
(abs(lambda - lower[1]) < 1e-6) | (abs(lambda - upper[1]) < 1e-6)
if (at_bound) cat("\nNote: solution is at/near a bound; consider widening upper bounds or raising sigma lower bound.\n")
# Plot comparison
op = par(no.readonly = TRUE); on.exit(par(op), add = TRUE)
barplot(rbind(emp, pred),
beside = TRUE, names.arg = paste0("M", 1:5),
main = "Empirical vs Predicted Raw Moments (Orders 1–5)\nConstrained two-Gaussian fit",
legend.text = c("Empirical", "Predicted"),
args.legend = list(x = "topleft", bty = "n"),
ylab = "value")
library(readr)
sales_data <- read_csv("Downloads/sales_data.csv")
View(sales_data)
summary(sales_data)
knitr::opts_chunk$set(echo = TRUE)
## Load packages
install.packages("tidyverse")   # run once
install.packages("skimr")       # run once
library(tidyverse)
library(skimr)
## 1. Load data
# Replace with your file path
sales = read.csv("sales.csv", stringsAsFactors = FALSE)
## Load packages
install.packages("tidyverse")   # run once
install.packages("skimr")       # run once
library(tidyverse)
library(skimr)
## 1. Load data
# Replace with your file path
sales = read.csv("sales.csv", stringsAsFactors = FALSE)
## 1. Load data
# Replace with your file path
sales = read.csv("sales.csv", stringsAsFactors = FALSE)
## Load packages
install.packages("tidyverse")   # run once
install.packages("skimr")       # run once
library(tidyverse)
library(skimr)
## 1. Load data
# Replace with your file path
sales = sales_data
## 2. Quick structure + preview
dim(sales)              # rows, columns
names(sales)            # column names
str(sales)              # types
head(sales, 10)         # first 10 rows
skim(sales)             # nice overview (types, missing, distributions)
## 3. Missing values
colSums(is.na(sales))   # missing by column
## 4. Basic numeric summaries
summary(sales)          # fast global summary
sales %>%
select(where(is.numeric)) %>%
summary()
## 5. Distributions for key numeric fields
# Example: sales_amount, quantity
sales %>%
ggplot(aes(x = sales_amount)) +
geom_histogram(bins = 30)
install.packages("skimr")
install.packages("tidyverse")
## 3. Missing values
colSums(is.na(sales))   # missing by column
str(sales)
## Packages (run install only once)
# install.packages("tidyverse")
# install.packages("skimr")
# install.packages("janitor")
library(tidyverse)
library(skimr)
library(janitor)
## 1. Load and clean column names (recommended)
# If you already have the data in an object (e.g. sales), skip read_csv and start at clean_names().
sales = read_csv("sales.csv", show_col_types = FALSE)
## Packages (run install only once)
# install.packages("tidyverse")
# install.packages("skimr")
# install.packages("janitor")
library(tidyverse)
library(skimr)
library(janitor)
## 1. Load and clean column names (recommended)
# If you already have the data in an object (e.g. sales), skip read_csv and start at clean_names().
# Convert "Store ID" etc. into snake_case: store_id, product_id, inventory_level, etc.
sales = sales %>%
clean_names()
## 2. Quick sanity checks
dim(sales)
names(sales)
glimpse(sales)
skim(sales)
problems(sales)   # if loaded via readr, see parsing issues
## 3. Basic data quality
# Missing values per column
sales %>%
summarise(across(everything(), ~ sum(is.na(.)))) %>%
pivot_longer(everything(),
names_to = "variable",
values_to = "missing_n") %>%
arrange(desc(missing_n))
# Duplicate rows check
sales %>%
summarise(duplicate_rows = n() - n_distinct(across(everything())))
# Range checks on key numeric fields
sales %>%
summarise(
min_units_sold = min(units_sold, na.rm = TRUE),
max_units_sold = max(units_sold, na.rm = TRUE),
min_price = min(price, na.rm = TRUE),
max_price = max(price, na.rm = TRUE),
min_discount = min(discount, na.rm = TRUE),
max_discount = max(discount, na.rm = TRUE),
min_inventory = min(inventory_level, na.rm = TRUE),
max_inventory = max(inventory_level, na.rm = TRUE)
)
## 4. Type tweaks (optional but useful)
sales = sales %>%
mutate(
date = as.Date(date),
store_id = as.factor(store_id),
product_id = as.factor(product_id),
category = as.factor(category),
region = as.factor(region),
weather_condition = as.factor(weather_condition),
seasonality = as.factor(seasonality),
promotion_flag = factor(promotion, levels = c(0, 1), labels = c("No Promo", "Promo")),
epidemic_flag = factor(epidemic, levels = c(0, 1), labels = c("No", "Yes"))
)
## 5. Univariate views
# Numeric distributions
numeric_cols = c("units_sold", "units_ordered", "price",
"discount", "inventory_level",
"competitor_pricing", "demand")
sales %>%
pivot_longer(all_of(numeric_cols),
names_to = "metric",
values_to = "value") %>%
ggplot(aes(x = value)) +
geom_histogram(bins = 30) +
facet_wrap(~ metric, scales = "free_x")
# Categorical frequencies
sales %>% count(category, sort = TRUE)
sales %>% count(region, sort = TRUE)
sales %>% count(weather_condition, sort = TRUE)
sales %>% count(seasonality, sort = TRUE)
sales %>% count(promotion_flag, sort = TRUE)
sales %>% count(epidemic_flag, sort = TRUE)
## 6. Time-series patterns
# Overall daily demand / sales volume
daily = sales %>%
group_by(date) %>%
summarise(
total_units_sold = sum(units_sold, na.rm = TRUE),
total_demand = sum(demand, na.rm = TRUE)
)
daily %>%
ggplot(aes(x = date, y = total_units_sold)) +
geom_line()
daily %>%
ggplot(aes(x = date, y = total_demand)) +
geom_line()
# Seasonality effect on units_sold
sales %>%
group_by(seasonality) %>%
summarise(
avg_units_sold = mean(units_sold, na.rm = TRUE),
avg_demand = mean(demand, na.rm = TRUE),
n = n()
) %>%
arrange(desc(avg_units_sold))
## 7. Store / Region / Category performance
# Top stores by units_sold
sales %>%
group_by(store_id) %>%
summarise(
total_units_sold = sum(units_sold, na.rm = TRUE),
total_demand = sum(demand, na.rm = TRUE)
) %>%
arrange(desc(total_units_sold)) %>%
head(15)
# Region vs Category heatmap
sales %>%
group_by(region, category) %>%
summarise(total_units_sold = sum(units_sold, na.rm = TRUE)) %>%
ggplot(aes(x = region, y = category, fill = total_units_sold)) +
geom_tile() +
scale_fill_continuous()
## 8. Relationships / drivers
# Price vs Units Sold
sales %>%
ggplot(aes(x = price, y = units_sold)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "lm", se = FALSE)
# Discount vs Units Sold
sales %>%
ggplot(aes(x = discount, y = units_sold)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "lm", se = FALSE)
# Competitor Pricing vs Units Sold
sales %>%
ggplot(aes(x = competitor_pricing, y = units_sold)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "lm", se = FALSE)
# Promotion flag vs Units Sold
sales %>%
group_by(promotion_flag) %>%
summarise(avg_units_sold = mean(units_sold, na.rm = TRUE),
avg_demand = mean(demand, na.rm = TRUE),
n = n())
## 9. Inventory vs Demand / Stock risk
sales %>%
mutate(gap = inventory_level - demand) %>%
summarise(
avg_gap = mean(gap, na.rm = TRUE),
understock_rate = mean(gap < 0, na.rm = TRUE)
)
# Understock by product
sales %>%
mutate(gap = inventory_level - demand) %>%
group_by(product_id) %>%
summarise(understock_rate = mean(gap < 0, na.rm = TRUE)) %>%
arrange(desc(understock_rate)) %>%
head(20)
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
eps = 0.01
m = 1:4
theta = c(m*pi/2 - eps, m*pi/2 + eps)
X = cbind(cos(theta), sin(theta))
fit10 = kmeans(X, centers = 4, nstart = 10, algorithm = "Lloyd")
fit10$tot.withinss                   # ≈ 0.0008
fit10$centers                        # near axes
sqrt(rowSums(fit10$centers^2))       # norms ≈ cos(eps)
setwd("~/Desktop/Project/postsales-churn-pred/backend/api")
library(plumber)
pr = plumb("main.R")
pr$run(host = "0.0.0.0", port = 8000)
